# -*- coding: utf-8 -*-
"""project2.ipynb

Automatically generated by Colab.

Original file is located atpython -V
    https://colab.research.google.com/drive/1po0CAIUK4CwfY4lTLh9peYW8uZgWZg8r
"""

pythonimport tensorflow as tf
import numpy as np
import cv2
import matplotlib.pyplot as plt

# Load the CIFAR-10 dataset (you can replace it with MNIST if you want)
(x_train, _), (x_test, _) = tf.keras.datasets.cifar10.load_data()

# Function to preprocess images
def preprocess_images(images, target_size=(64, 64)):
    # Convert to grayscale
    gray_images = np.array([cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) for img in images])

    # Resize to 64x64 pixels
    resized_images = np.array([cv2.resize(img, target_size) for img in gray_images])

    # Normalize pixel values to the range [0, 1]
    normalized_images = resized_images.astype(np.float32) / 255.0

    return normalized_images

# Preprocess the CIFAR-10 images (taking a subset of the dataset for demonstration)
processed_images = preprocess_images(x_train[:100])  # Using the first 100 images for demonstration

# Display the processed image (optional)
plt.imshow(processed_images[0], cmap='gray')
plt.title("Processed Image")
plt.show()

# Verify the shape and range of processed images
print("Processed image shape:", processed_images.shape)
print("Min pixel value:", processed_images.min())
print("Max pixel value:", processed_images.max())

import numpy as np
import cv2
import matplotlib.pyplot as plt
import tensorflow as tf

# Load CIFAR-10 dataset (or MNIST if needed)
(x_train, _), (_, _) = tf.keras.datasets.cifar10.load_data()

# Convert to grayscale, resize to 64x64, and normalize
def preprocess_images(images, target_size=(64, 64)):
    gray_images = np.array([cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) for img in images])
    resized_images = np.array([cv2.resize(img, target_size) for img in gray_images])
    normalized_images = resized_images.astype(np.float32) / 255.0
    return normalized_images

processed_images = preprocess_images(x_train[:25])  # First 25 images for demo

# Function to apply random transformations
def augment_image(image):
    choice = np.random.choice(["flip", "rotate", "noise"])

    if choice == "flip":
        return cv2.flip(image, 1)  # Horizontal flip
    elif choice == "rotate":
        angle = np.random.randint(-30, 30)
        (h, w) = image.shape[:2]
        matrix = cv2.getRotationMatrix2D((w//2, h//2), angle, 1.0)
        return cv2.warpAffine(image, matrix, (w, h))
    elif choice == "noise":
        noise = np.random.normal(0, 0.1, image.shape)  # Gaussian noise
        return np.clip(image + noise, 0, 1)

# Apply transformations
augmented_images = np.array([augment_image(img) for img in processed_images])

# Display the transformed images in a 5x5 grid
fig, axes = plt.subplots(5, 5, figsize=(10, 10))
for i, ax in enumerate(axes.flat):
    ax.imshow(augmented_images[i], cmap="gray")
    ax.axis("off")

plt.suptitle("5x5 Grid of Transformed Images", fontsize=15)
plt.show()

import tensorflow as tf
from tensorflow.keras import layers, models

# Define the encoder
def build_autoencoder():
    input_img = layers.Input(shape=(64, 64, 1))  # Input grayscale image

    # Encoder: Compress to latent space of size 32
    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)
    x = layers.MaxPooling2D((2, 2), padding='same')(x)
    x = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(x)
    x = layers.MaxPooling2D((2, 2), padding='same')(x)
    encoded = layers.Flatten()(x)
    encoded = layers.Dense(32, activation="relu")(encoded)  # Latent space

    # Decoder: Reconstruct the image
    x = layers.Dense(16 * 16 * 16, activation="relu")(encoded)
    x = layers.Reshape((16, 16, 16))(x)
    x = layers.Conv2DTranspose(16, (3, 3), activation='relu', padding='same')(x)
    x = layers.UpSampling2D((2, 2))(x)
    x = layers.Conv2DTranspose(32, (3, 3), activation='relu', padding='same')(x)
    x = layers.UpSampling2D((2, 2))(x)
    decoded = layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)

    autoencoder = models.Model(input_img, decoded)
    autoencoder.compile(optimizer='adam', loss='mse')
    return autoencoder

autoencoder = build_autoencoder()
autoencoder.summary()

# Prepare dataset for training
processed_images = np.expand_dims(processed_images, axis=-1)  # Add channel dimension

# Train the autoencoder
autoencoder.fit(processed_images, processed_images, epochs=10, batch_size=32, shuffle=True)

# Generate reconstructed images
reconstructed_images = autoencoder.predict(processed_images[:5])

# Display original vs reconstructed images
fig, axes = plt.subplots(2, 5, figsize=(10, 4))
for i in range(5):
    # Original
    axes[0, i].imshow(processed_images[i].squeeze(), cmap="gray")
    axes[0, i].axis("off")

    # Reconstructed
    axes[1, i].imshow(reconstructed_images[i].squeeze(), cmap="gray")
    axes[1, i].axis("off")

axes[0, 0].set_title("Original Images")
axes[1, 0].set_title("Reconstructed Images")
plt.show()

def apply_edge_detection(image):
    return cv2.Canny((image * 255).astype(np.uint8), 100, 200)  # Convert back to 0-255 range

edge_images = np.array([apply_edge_detection(img) for img in processed_images[:5]])

# Display Edge Detected Images
fig, axes = plt.subplots(1, 5, figsize=(10, 2))
for i, ax in enumerate(axes):
    ax.imshow(edge_images[i], cmap="gray")
    ax.axis("off")

plt.suptitle("Edge Detection Applied", fontsize=15)
plt.show()
